knitr::opts_chunk$set(echo = TRUE)
K = 2
D = 2
mu_star1 = c(10, 0)
mu_star2 = c(2, 2)
Sigma_star1 = matrix(c(1, 0.2, 0.2, 1), nrow = 2)
Sigma_star2 = matrix(c(1, 0.5, 0.5, 0.7), nrow = 2)
mus_star = matrix(mu_star1, mu_star2, nrow = 2)
pi_star = c(0.7, 0.3)
View(mus_star)
View(mus_star)
K = 2
D = 2
mu_star1 = c(10, 0)
mu_star2 = c(2, 2)
Sigma_star1 = matrix(c(1, 0.2, 0.2, 1), nrow = 2)
Sigma_star2 = matrix(c(1, 0.5, 0.5, 0.7), nrow = 2)
mus_star = matrix(c(mu_star1, mu_star2), nrow = 2)
pi_star = c(0.7, 0.3)
View(mus_star)
View(mus_star)
K = 2
D = 2
mu_star1 = c(10, 0)
mu_star2 = c(2, 2)
Sigma_star1 = matrix(c(1, 0.2, 0.2, 1), nrow = 2)
Sigma_star2 = matrix(c(1, 0.5, 0.5, 0.7), nrow = 2)
mus_star = matrix(c(mu_star1, mu_star2), nrow = 2)
Sigmas_star = matrix(c(Sigma_star1, Sigma_star2), nrow = 2)
pi_star = c(0.7, 0.3)
View(Sigmas_star)
K = 2
D = 2
mus_star = array(dim = (D, K))
K = 2
D = 2
mus_star = array(dim = (D, K))
K = 2
D = 2
mus_star = array(dim = (D, K))
help(array)
K = 2
D = 2
mus_star = array(dim = c(D, K))
Sigmas_star = array(dim = c(D, D, K))
pi_star = c(0.7, 0.3)
mus_star[, 1] = c(10, 0)
mus_star[, 2] = c(2, 2)
Sigmas_star[,, 1] = matrix(c(1, 0.2, 0.2, 1), nrow = 2)
Sigmas_star[,, 2] = matrix(c(1, 0.5, 0.5, 0.7), nrow = 2)
K = 2
D = 2
mus_star = array(dim = c(D, K))
Sigmas_star = array(dim = c(D, D, K))
pi_star = c(0.7, 0.3)
mus_star[, 1] = c(10, 0)
mus_star[, 2] = c(2, 2)
Sigmas_star[,, 1] = matrix(c(1, 0.2, 0.2, 1), nrow = 2)
Sigmas_star[,, 2] = matrix(c(1, 0.5, 0.5, 0.7), nrow = 2)
View(mus_star)
N = 100
Z = rmultinom(N, 1, pi_star)
N = 100
Z = rmultinom(N, 1, pi_star)
X1 = MASS::mvrnorm(n = N, mus_star, Sigmas_star)
N = 100
Z = rmultinom(N, 1, pi_star)
X1 = MASS::mvrnorm(n = N/2, mus_star[, 1], Sigmas_star[,, 1])
X2 = MASS::mvrnorm(n = N/2, mus_star[, 2], Sigmas_star[,, 2])
X = rbind(X1, X2)
plot(X1, col = "red")
plot(X2, col = "blue")
plot(X1, col = "red") + plot(X2, col = "blue")
plot(X)
point(X1, "red")
point(X1, "red")
plot()
N = 200
Z = t(rmultinom(N, 1, pi_star))
N_k = colSums(Z)
X = matrix(0, nrow = N, ncol = D)
for (k in 1:K) {
X[Z[, k] == 1, ] = MASS::mvrnorm(N_k[k], mus_star[, k], Sigmas_star[,, k])
}
N = 200
Z = t(rmultinom(N, 1, pi_star))
N_k = colSums(Z)
X = matrix(0, nrow = N, ncol = D)
for (k in 1:K) {
X[Z[, k] == 1, ] = MASS::mvrnorm(N_k[k], mus_star[, k], Sigmas_star[,, k])
}
max.col(Z)
plot(X1, col = "red")
points(X2, col = "blue")
plot(X, col = max.col(Z))
plot(X, col = max.col(Z))
lines(ellipse(Sigmas_star[,,1], center=mus_star[,1]), col=1)
library(MASS)
plot(X, col = max.col(Z))
lines(ellipse(Sigmas_star[,,1], center=mus_star[,1]), col=1)
help(ellipse)
library(cluster)
plot(X, col = max.col(Z))
lines(ellipse(Sigmas_star[,,1], center=mus_star[,1]), col=1)
library(plotrix)
install.packages("plotrix")
library(plotrix)
plot(X, col = max.col(Z))
lines(ellipse(Sigmas_star[,,1], center=mus_star[,1]), col=1)
library(plotrix)
plot(X, col = max.col(Z))
install.packages("bayess")
knitr::opts_chunk$set(echo = TRUE)
data("caterpillar", package = "bayess")
y = log(caterpillar$y)
X = as.matrix(caterpillar[,1:8])
data("caterpillar", package = "bayess")
y = log(caterpillar$y)
X = as.matrix(caterpillar[,1:8])
View(caterpillar)
View(X)
View(caterpillar)
help(lm)
View(caterpillar)
reg_lin = lm(data = caterpillar, formula = log(y) ~ colnames(caterpillar)[1:8])
reg_lin = lm(data = caterpillar, formula = log(y) ~ caterpillar[1:8])
caterpillar$y = log(caterpillar$y)
reg_lin = lm(data = caterpillar, formula = y ~ caterpillar[1:8])
data("caterpillar", package = "bayess")
y = log(caterpillar$y)
X = as.matrix(caterpillar[,1:8])
caterpillar$y = log(caterpillar$y)
reg_lin = lm(data = caterpillar, formula = y ~ caterpillar[1:8])
reg_lin = lm(data = caterpillar, formula = y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8)
View(reg_lin)
lm(y ~ X)
reg_lin = lm(y ~ X)
reg_lin_bis = lm(data = caterpillar, formula = y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8)
reg_lin = lm(y ~ X)
summary(reg_lin)
summary(reg_lin)
library(bayess)
data("caterpillar")
y = log(caterpillar$y)
X = as.matrix(caterpillar[,1:8])
caterpillar$y = log(caterpillar$y)
library(bayess)
data("caterpillar", package = "bayess")
y = log(caterpillar$y)
X = as.matrix(caterpillar[,1:8])
caterpillar$y = log(caterpillar$y)
reg_bay = BayesReg(y, X)
setwd("C:\Users\natha\Documents\Git\4th-year-Polytech\Regression lineaire\Regression logistique\TP 2.1 Courbe ROC")
setwd("C:\\Users\\natha\\Documents\\Git\\4th-year-Polytech\\Regression lineaire\\Regression logistique\\TP 2.1 Courbe ROC")
getwd()
dir()
library(gridExtra)# pour faire des graphiques multiples
install.packages("gridExtra")
library(gridExtra)# pour faire des graphiques multiples
load("intestin.Rdata")
attach(intestin)
names(intestin)
head(intestin,10)
View(intestin)
summary(intestin,10)
# pr?valence (de la table) = nombre de cas sur le nombre total d'individus
cas=table(intestin$popind)
cas
prevalence=cas[2]/(cas[1]+cas[2])
prevalence
# -----------------------
# Analyse descriptive
# -----------------------
library(ggplot2)
library(GGally) # pour utiliser pairs am?lior?
ggpairs(data=intestin,columns=(1:3), aes(color=popind)) +
ggtitle("Facteurs pr?dictifs de la maladie")
# Mod?le logistique
model <- glm(popind ~ alb + tp + totscore, family="binomial"(link="logit"))
summary(model)
# anova donnee des tests de type I
anova(model,test="Chisq")
# Test de type II
library(car)
Anova(model, type="2")
# inversion des deux premi?res variables
modelbis <- glm(popind ~   tp + alb + totscore, family="binomial"(link="logit"))
anova(modelbis,test="Chisq")
library(MASS)
# selection de variable backward
#AIC
stepAIC(model, k=2,selection="backward")
# BIC : log(length(popind))= nombr d'individus dans la base
stepAIC(model, k=log(length(popind)),selection="backward")
# selection de variable forward
# il faut d?finir le mod?le de d?part , ici mod0
mod0 <- glm( popind ~ 1, family="binomial" (link=logit))
#AIC
modAICF <- stepAIC(mod0,scope=list(upper=model), k=2,selection="forward")
summary(modAICF)
# BIC
stepAIC(mod0,scope=list(upper=model), k=log(length(popind)),selection="forward")
# compl?ment
# Autre syntaxe avec poly() mais ne permet pas de faire du choix de mod?le.
modelplus <- glm(popind ~ poly(alb,tp,totscore, degree=2, raw=TRUE), family="binomial"(link="logit"))
summary(modelplus)
stepAIC(modelplus, k=2,selection="backward")
stepAIC(mod0, scope=list(upper=modelplus), k=2,selection="forward")
stepAIC(modelplus, k=log(length(popind)),selection="backward")
# On retient le mod?le avec alb et totscore
modfinal <- glm(popind ~ alb + totscore, family="binomial" (link=logit))
# analyse en type I
anova(modfinal, test="Chisq")
# analyse en type 2
Anova(modfinal, type=2)
# ----------------------
# Analyse pr?dictive par LOOCV
# ----------------------
library(ROCR) # pour construire des courbes ROC, PRC etc.
install.packages("ROCR")
# initialisation du vecteur des predictions
score.cv <- rep(0,length(intestin$popind))
# Leave one out cross validation
# cr?ation d'un vecteur contenant les scores pr?dits.
for (i in 1:length(popind)){
data.train=intestin[-i,]
data.test=intestin[i,]
mod <- glm(data=data.train,popind ~ alb + totscore, family="binomial" (link=logit))
score.cv[i] <- predict(mod,data.test, type="response")
}
# Leave one out cross validation
# cr?ation d'un vecteur contenant les scores pr?dits.
for (i in 1:length(popind)){
data.train=intestin[-i,]
data.test=intestin[i,]
mod <- glm(data=data.train,popind ~ alb + totscore, family="binomial" (link=logit))
score.cv[i] <- predict(mod,data.test, type="response")
}
score.cv
pred.test = prediction(score.cv, labels = popind)
pred.test = predict(score.cv, labels = popind)
# il vaut mieux faire
popindnum <- as.numeric(levels(popind))[popind]
popindnum
# met ensemble les scores (pr?diction)
# et la valeur r?elle de y (popind)
pred.test <- prediction(score.cv, labels=popindnum)
pred.test
# met ensemble les scores (pr?diction)
# et la valeur r?elle de y (popind)
pred.test <- mod.predict(score.cv, labels=popindnum)
# met ensemble les scores (pr?diction)
# et la valeur r?elle de y (popind)
pred.test <- predict(mod, score.cv, labels=popindnum)
?prediction
# Calcul de l'AUC
AUC <- attributes(performance(pred.test, "auc"))$y.values
# met ensemble les scores (pr?diction)
# et la valeur r?elle de y (popind)
pred.test <- predict(mod, score.cv)
# met ensemble les scores (pr?diction)
# et la valeur r?elle de y (popind)
pred.test <- predict(mod, score)
# met ensemble les scores (pr?diction)
# et la valeur r?elle de y (popind)
pred.test <- predict(mod, score.cv, labels=popindnum)
source("~/Git/4th-year-Polytech/Regression lineaire/Regression logistique/TP 2.1 Courbe ROC/intestin.R", echo=TRUE)
setwd("C:\\Users\\natha\\Documents\\Git\\4th-year-Polytech\\Regression lineaire\\Regression logistique\\TP 2.1 Courbe ROC")
getwd()
dir()
library(gridExtra)# pour faire des graphiques multiples
load("intestin.Rdata")
attach(intestin)
names(intestin)
head(intestin,10)
summary(intestin,10)
# albumin : Score de quantit? d'albumine dans le corps.
# tp : score de quantit? totale de prot?ine dans le corps (albumine+globuline).
# totscore ? : score K-G.
# popind : pr?sence ou non d'occlusion intestinale.
# pr?valence (de la table) = nombre de cas sur le nombre total d'individus
cas=table(intestin$popind)
cas
prevalence=cas[2]/(cas[1]+cas[2])
prevalence
# -----------------------
# Analyse descriptive
# -----------------------
library(ggplot2)
library(GGally) # pour utiliser pairs am?lior?
ggpairs(data=intestin,columns=(1:3), aes(color=popind)) +
ggtitle("Facteurs pr?dictifs de la maladie")
# On voit que "tp" et "alb" sont fortement corr?l?es
# -----------------------
# Mod?lisation
# -----------------------
# Mod?le logistique
model <- glm(popind ~ alb + tp + totscore, family="binomial"(link="logit"))
summary(model)
# anova donnee des tests de type I
anova(model,test="Chisq")
# Test de type II
library(car)
Anova(model, type="2")
# inversion des deux premi?res variables
modelbis <- glm(popind ~   tp + alb + totscore, family="binomial"(link="logit"))
anova(modelbis,test="Chisq")
library(MASS)
# selection de variable backward
#AIC
stepAIC(model, k=2,selection="backward")
# BIC : log(length(popind))= nombr d'individus dans la base
stepAIC(model, k=log(length(popind)),selection="backward")
# selection de variable forward
# il faut d?finir le mod?le de d?part , ici mod0
mod0 <- glm( popind ~ 1, family="binomial" (link=logit))
#AIC
modAICF <- stepAIC(mod0,scope=list(upper=model), k=2,selection="forward")
summary(modAICF)
# BIC
stepAIC(mod0,scope=list(upper=model), k=log(length(popind)),selection="forward")
# compl?ment
# Autre syntaxe avec poly() mais ne permet pas de faire du choix de mod?le.
modelplus <- glm(popind ~ poly(alb,tp,totscore, degree=2, raw=TRUE), family="binomial"(link="logit"))
summary(modelplus)
stepAIC(modelplus, k=2,selection="backward")
stepAIC(mod0, scope=list(upper=modelplus), k=2,selection="forward")
stepAIC(modelplus, k=log(length(popind)),selection="backward")
# fin du compl?ment
# On retient le mod?le avec alb et totscore
modfinal <- glm(popind ~ alb + totscore, family="binomial" (link=logit))
# analyse en type I
anova(modfinal, test="Chisq")
# analyse en type 2
Anova(modfinal, type=2)
# ----------------------
# Analyse pr?dictive par LOOCV
# ----------------------
library(ROCR) # pour construire des courbes ROC, PRC etc.
# initialisation du vecteur des predictions
score.cv <- rep(0,length(intestin$popind))
# Leave one out cross validation
# cr?ation d'un vecteur contenant les scores pr?dits.
for (i in 1:length(popind)){
data.train=intestin[-i,]
data.test=intestin[i,]
mod <- glm(data=data.train,popind ~ alb + totscore, family="binomial" (link=logit))
score.cv[i] <- predict(mod,data.test, type="response")
}
score.cv
# transformation de popind en num?rique
# !! attention is.numric ne marche pas (rajoute +1 au vecteur)
# et je ne sais as pourquoi
as.numeric(popind)
# il vaut mieux faire
popindnum <- as.numeric(levels(popind))[popind]
popindnum
# met ensemble les scores (pr?diction)
# et la valeur r?elle de y (popind)
pred.test <- prediction(score.cv, labels=popindnum)
pred.test
class(pred.test)
# Calcul de l'AUC
AUC <- attributes(performance(pred.test, "auc"))$y.values
AUC
AUC %>% unlist
View(AUC)
View(AUC)
# Cr?ation de la courbe ROC
# calcule la performance
# tpr=true positive rate  fpr=false positive  rate
# "tpr"="sens", "fpr"= 1-spec
perfROC.cv <- performance(pred.test, measure="tpr", x.measure="fpr")
# our simplemnet
perfROC.cv <- performance(pred.test, "tpr", "fpr")
# courbe ROC
plot(perfROC.cv,window.size=50,col="red")
# affichage des cutoff (test.ce x=taille des caract?res)
plot(perfROC.cv, print.cutoffs.at=seq(0,1,by=0.1),text.cex=1.5,col="red")
# avec ggplot
sensibilite=perfROC.cv@y.values[[1]]
specificite=perfROC.cv@x.values[[1]]
# avec ggplot
sensibilite=perfROC.cv@y.values[[1]]
specificite=perfROC.cv@x.values[[1]]
ggplot(data = NULL, aes(x = specificite, y=sensibilite))+
geom_line(color = "red", size = 1.5) +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "grey") +
labs(title = "Courbe ROC", x = "sensibilit?", y = "sp?cificit?") +
theme_minimal()
score.appr <- predict(modfinal, type="response")
pred.appr <- prediction(score.appr, labels=popind)
perfROC.appr <- performance(pred.appr, "tpr", "fpr")
plot(perfROC.appr, col="blue", main="courbe ROC")
plot(perfROC.cv, col="red", add=TRUE)
# courbe PRC
# "prec"="tpr"
# "rec"="ppv"
# La courbe est invers?e : probl?me d'impl?mentation ?
perfPRC.cv <- performance(pred.test, measure="prec", x.measure="rec")
plot(perfPRC.cv, print.cutoffs.at=seq(0,1,by=0.1),text.cex=1.5,colorize=T)
plot(perfPRC.cv, col ="red")
?performance
# courbe Lift
# "prec"="tpr"
# "rec"="ppv"
perfLIFT.cv <- performance(pred.test, "lift", "rpp")
plot(perfLIFT.cv, print.cutoffs.at=seq(0,1,by=0.1),text.cex=1.5,colorize=T)
plot(perfLIFT.cv, col ="red")
sort(score.cv)
##########################
# A VOIR PLUS TARD   cv avec caret
library(caret) # pour faire de la validation crois?e
# pour sp?cifier la m?thode que l'on va utiliser dans train (ici du  One Leave-out CV)
train.control <- trainControl(method = "LOOCV")
# calcul des scores par CV (par d?faut avec glm c'est une r?gression logistique)
predmod <- train(popind ~alb+totscore , data=intestin, method="glm", trControl = train.control)
# calcule les scores sur l'?chantillon d'apprentissage et de test
scorecv <- predict(predmod, type="prob") # donne les probas de y=0 et y=1
scorecv[,1]<-NULL  # pour ne garder que la proba de Y=1
head(scorecv)
# met ensemble les scores (pr?diction) et la valeur r?elle de y
predtest <- prediction(scorecv, labels=popind)
# affichage des cutoff (test.ce x=taille des caract?res)
plot(perfcv, print.cutoffs.at=seq(0,1,by=0.1),text.cex=1.5,colorize=T)
plot(perfcv,window.size=50,colorize=F,add=TRUE)
setwd("C:\\Users\\natha\\Documents\\Git\\4th-year-Polytech\\Regression lineaire\\Regression logistique\\TP 3 Pima")
getwd()
dir()
load("pima.Rdata")
View(pima)
names(pima)
head(pima, 10)
summary(pima)
pima$test = as.factor(pima$test)
summary(test)
summary(pima)
str(pima)
# Prévalence
a = table(pima$test)
a
prevalence = a[2]/(a[1] + a[2])
prevalence
sum(a)
prevalence = a[2]/(sum(a))
prevalence
summary(pima$age)
hist(pima$age)
pima$diastolic[pima$diastolic == 0] = NA
pima$glucose[pima$glucose == 0] = NA
pima$triceps[pima$triceps == 0] = NA
pima$insulin[pima$insulin == 0] = NA
pima$bma[pima$bma == 0] = NA
summary(pima)
pima$bmi[pima$bmi == 0] = NA
summary(pima)
# Premier modèle de Reg Log
result = glm(test ~ diastolic + glucose + triceps + insulin + bmi + diabetes, family = binomial (link = logit), data = pima)
summary(result)
# On décide d'enlever "insuline"
result = glm(test ~ diastolic + glucose + triceps + bmi + diabetes,
family = binomial (link = logit), data = pima)
summary(result)
ggpairs(data = pima, columns = (1:8), aes(color = test))
ggpairs(data = pima, columns = (1:8), aes(color = test))
result = glm(test ~ diastolic + glucose + triceps + insulin + bmi + diabetes,
family = binomial (link = logit), data = pima)
library(MASS)
resAIC = stepAIC(result, direction = "backward", trace = TRUE)
# Retirer les lignes avec des valeurs NA manquantes
pima2 = na.omit(pima)
result2 = glm(test ~ diastolic + glucose + triceps + insulin + bmi + diabetes,
family = binomial (link = logit), data = pima2)
resAIC = stepAIC(result2, direction = "backward", trace = TRUE)
summary(resAIC)
pima3 = pima[, c(2, 6, 7, 9)]
head(pima3)
pima4 = na.omit(pima3)
result3 = glm(formula = test ~ glucose + bmi + diabetes, family = binomial(link = logit),
data = pima4)
summary(result3)
plot(result3)
# Odd ratios
OR = exp(result3$coef)
OR
IC
# Intervalle de confiance
IC = confint(result3, level = 0.95)
IC

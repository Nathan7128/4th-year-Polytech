{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model permettant de classifier des reviews de film, avec kfold\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB dataset importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries importing\n",
    "from keras.datasets import imdb\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = 4000)\n",
    "# num_words = dictionnary len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = pad token\n",
    "# 1 = beginning character of the review\n",
    "# 2 = words that aren't in the dictionnary : \"oov_char\" parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the review structure in 200 characters\n",
    "x_train_padded = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen = 200)\n",
    "x_test_padded = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 200), (25000, 200))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_padded.shape, x_test_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining validation and test data\n",
    "x_data_val, y_data_val = x_test_padded[:int(len(x_test_padded)/2)], y_test[:int(len(x_test_padded)/2)]\n",
    "x_data_test, y_data_test = x_test_padded[int(len(x_test_padded)/2):], y_test[int(len(x_test_padded)/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12500, 200), (12500, 200))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data_val.shape, x_data_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries importing\n",
    "# Keras layers\n",
    "from keras.layers import Input, Embedding, MultiHeadAttention, LayerNormalization, Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Concatenate\n",
    "\n",
    "# Keras model\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "# Pour obtenir une image png du modèle\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# Optimizers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Loss functions\n",
    "from keras.losses import BinaryCrossentropy\n",
    "\n",
    "# Metrics\n",
    "from keras.metrics import BinaryAccuracy\n",
    "\n",
    "# Kfold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension de l'espace ou seront projetés les tokens par l'embedding_layer\n",
    "token_projection_dim = 32\n",
    "\n",
    "\n",
    "# Fonction permettant de définir le modèle\n",
    "\n",
    "def create_model(X, y, X_v, y_v, count) :\n",
    "\n",
    "    # Définition des fonctions utilisées dans le callbacks\n",
    "    callbacks = [\n",
    "        # Fonction qui permet de stopper le modèle si la binary accuracy n'augmente pas après 2 epochs pour les données de validations\n",
    "        EarlyStopping(monitor = \"val_binary_accuracy\", patience = 2, mode = 'max'),\n",
    "        # Fonction qui permet d'enregistrer le meilleur modèle dans les fichiers\n",
    "        ModelCheckpoint(\n",
    "            filepath = \"model/my_best_model\" + str(count) + \".keras\",\n",
    "            monitor = \"val_binary_accuracy\",\n",
    "            mode = \"max\",\n",
    "            save_best_only = True,\n",
    "            verbose = 1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    input_layer = Input(shape = [200,], dtype = 'int64')\n",
    "\n",
    "    embedding_layer = Embedding(input_dim = 4000, input_length = 200, output_dim = token_projection_dim)(input_layer)\n",
    "\n",
    "\n",
    "    # Transformers\n",
    "\n",
    "    # Couche d'attention\n",
    "    MHA_layer_T = MultiHeadAttention(num_heads = 2, dropout = 0.1, key_dim = token_projection_dim)(embedding_layer, embedding_layer, embedding_layer)\n",
    "\n",
    "    normalization_layer1_T = LayerNormalization(epsilon = 1e-6)(embedding_layer + MHA_layer_T)\n",
    "\n",
    "    dense_layer1_T = Dense(token_projection_dim, activation = \"tanh\")(normalization_layer1_T)\n",
    "\n",
    "    dense_layer2_T = Dense(token_projection_dim, activation = \"tanh\")(dense_layer1_T)\n",
    "\n",
    "    normalization_layer2_T = LayerNormalization(epsilon = 1e-6)(normalization_layer1_T + dense_layer2_T)\n",
    "\n",
    "    flatten_layer_T = Flatten()(normalization_layer2_T)\n",
    "\n",
    "\n",
    "    # LSTM\n",
    "\n",
    "    LSTM_layer_LSTM = LSTM(5)(embedding_layer)\n",
    "\n",
    "    dropout_layer_LSTM = Dropout(0.5)(LSTM_layer_LSTM)\n",
    "\n",
    "\n",
    "    # CNN\n",
    "\n",
    "    conv_layer_CNN = Conv1D(16, 3, input_shape = (200, 64))(embedding_layer)\n",
    "\n",
    "    dropout_layer_CNN = Dropout(0.5)(conv_layer_CNN)\n",
    "\n",
    "    pooling_layer_CNN = MaxPooling1D(2)(dropout_layer_CNN)\n",
    "\n",
    "    flatten_layer_CNN = Flatten()(pooling_layer_CNN)\n",
    "\n",
    "\n",
    "    # Concatenate\n",
    "\n",
    "    concatenate_layer = Concatenate()([flatten_layer_T, dropout_layer_LSTM, flatten_layer_CNN])\n",
    "\n",
    "\n",
    "    # Output\n",
    "\n",
    "    output_layer = Dense(1, activation = \"sigmoid\")(concatenate_layer)\n",
    "\n",
    "    model = Model(inputs = input_layer, outputs = output_layer)\n",
    "\n",
    "    model.compile(optimizer = Adam(learning_rate = 0.001), loss = BinaryCrossentropy(), metrics = [BinaryAccuracy()])\n",
    "\n",
    "    # Model fitting\n",
    "    model.fit(X, y, batch_size = 32, epochs = 3, validation_data = (X_v, y_v), callbacks = callbacks)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kfold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natha\\Documents\\GitHub\\4th-year-Polytech\\Deep Learning\\env_deep_learning\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "c:\\Users\\natha\\Documents\\GitHub\\4th-year-Polytech\\Deep Learning\\env_deep_learning\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m520/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - binary_accuracy: 0.5767 - loss: 0.7537\n",
      "Epoch 1: val_binary_accuracy improved from -inf to 0.82217, saving model to model/my_best_model0.keras\n",
      "\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 60ms/step - binary_accuracy: 0.5770 - loss: 0.7531 - val_binary_accuracy: 0.8222 - val_loss: 0.3809\n",
      "Epoch 2/3\n",
      "\u001b[1m520/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - binary_accuracy: 0.9163 - loss: 0.2070\n",
      "Epoch 2: val_binary_accuracy improved from 0.82217 to 0.83093, saving model to model/my_best_model0.keras\n",
      "\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 54ms/step - binary_accuracy: 0.9163 - loss: 0.2070 - val_binary_accuracy: 0.8309 - val_loss: 0.4050\n",
      "Epoch 3/3\n",
      "\u001b[1m520/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - binary_accuracy: 0.9801 - loss: 0.0676\n",
      "Epoch 3: val_binary_accuracy improved from 0.83093 to 0.84833, saving model to model/my_best_model0.keras\n",
      "\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 55ms/step - binary_accuracy: 0.9801 - loss: 0.0677 - val_binary_accuracy: 0.8483 - val_loss: 0.4192\n",
      "Epoch 1/3\n",
      "\u001b[1m520/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - binary_accuracy: 0.5685 - loss: 0.7713\n",
      "Epoch 1: val_binary_accuracy improved from -inf to 0.84231, saving model to model/my_best_model1.keras\n",
      "\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 62ms/step - binary_accuracy: 0.5689 - loss: 0.7707 - val_binary_accuracy: 0.8423 - val_loss: 0.3623\n",
      "Epoch 2/3\n",
      "\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - binary_accuracy: 0.9166 - loss: 0.2131\n",
      "Epoch 2: val_binary_accuracy did not improve from 0.84231\n",
      "\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 62ms/step - binary_accuracy: 0.9166 - loss: 0.2131 - val_binary_accuracy: 0.8398 - val_loss: 0.3900\n",
      "Epoch 3/3\n",
      "\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - binary_accuracy: 0.9827 - loss: 0.0647\n",
      "Epoch 3: val_binary_accuracy did not improve from 0.84231\n",
      "\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - binary_accuracy: 0.9827 - loss: 0.0647 - val_binary_accuracy: 0.8391 - val_loss: 0.4314\n",
      "Epoch 1/3\n",
      "\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - binary_accuracy: 0.6327 - loss: 0.6600\n",
      "Epoch 1: val_binary_accuracy improved from -inf to 0.83127, saving model to model/my_best_model2.keras\n",
      "\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 126ms/step - binary_accuracy: 0.6329 - loss: 0.6597 - val_binary_accuracy: 0.8313 - val_loss: 0.3701\n",
      "Epoch 2/3\n",
      "\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - binary_accuracy: 0.9205 - loss: 0.2035\n",
      "Epoch 2: val_binary_accuracy improved from 0.83127 to 0.84483, saving model to model/my_best_model2.keras\n",
      "\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 121ms/step - binary_accuracy: 0.9205 - loss: 0.2035 - val_binary_accuracy: 0.8448 - val_loss: 0.3664\n",
      "Epoch 3/3\n",
      "\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - binary_accuracy: 0.9853 - loss: 0.0569\n",
      "Epoch 3: val_binary_accuracy did not improve from 0.84483\n",
      "\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 101ms/step - binary_accuracy: 0.9853 - loss: 0.0569 - val_binary_accuracy: 0.8309 - val_loss: 0.4646\n"
     ]
    }
   ],
   "source": [
    "# split data into 3 folds\n",
    "n_folds = 3\n",
    "kfold = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = 0)\n",
    "\n",
    "# Compteur itératif\n",
    "i = 0\n",
    "\n",
    "# Liste contenant les historiques des entrainements des modèles\n",
    "history_list = []\n",
    "\n",
    "for train_index, test_index in kfold.split(x_train_padded, y_train) :\n",
    "    # create model\n",
    "    model = create_model(x_train_padded[train_index], y_train[train_index], x_train_padded[test_index], y_train[test_index], i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère l'enmseble des meilleurs modèles dans une liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model = []\n",
    "for i in range (n_folds) :\n",
    "    all_model.append(load_model(\"model/my_best_model\" + str(i) + \".keras\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prédiction sur les données de tests avec les modèles chargés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step\n",
      "accuracy :  0.87016\n"
     ]
    }
   ],
   "source": [
    "model_predict_moyenne = all_model[0].predict(x_data_test)\n",
    "\n",
    "for i in range(1, n_folds) :\n",
    "    model_predict_moyenne += all_model[i].predict(x_data_test)\n",
    "\n",
    "model_predict_moyenne = np.round(model_predict_moyenne/n_folds)\n",
    "\n",
    "print(\"accuracy : \", 1 - (np.sum(np.abs(model_predict_moyenne[:, 0] - y_data_test))/len(y_data_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient une accuracy de 87% sur les données de test, ce qui est supérieur à l'accuracy obtenu sans kfold, on peut donc en déduire que la kfold permet d'obtenir un meilleur modèle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

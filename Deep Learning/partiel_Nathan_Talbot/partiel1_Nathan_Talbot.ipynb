{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model permettant de classifier des reviews de film, sans kfold\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB dataset importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries importing\n",
    "from keras.datasets import imdb\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = 4000)\n",
    "# num_words = dictionnary len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = pad token\n",
    "# 1 = beginning character of the review\n",
    "# 2 = words that aren't in the dictionnary : \"oov_char\" parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the review structure in 200 characters\n",
    "x_train_padded = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen = 200)\n",
    "x_test_padded = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 200), (25000, 200))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_padded.shape, x_test_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining validation and test data\n",
    "x_data_val, y_data_val = x_test_padded[:int(len(x_test_padded)/2)], y_test[:int(len(x_test_padded)/2)]\n",
    "x_data_test, y_data_test = x_test_padded[int(len(x_test_padded)/2):], y_test[int(len(x_test_padded)/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12500, 200), (12500, 200))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data_val.shape, x_data_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries importing\n",
    "# Keras layers\n",
    "from keras.layers import Input, Embedding, MultiHeadAttention, LayerNormalization, Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Concatenate\n",
    "\n",
    "# Keras model\n",
    "from keras.models import Model\n",
    "\n",
    "# Pour obtenir une image png du modèle\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# Optimizers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Loss functions\n",
    "from keras.losses import BinaryCrossentropy\n",
    "\n",
    "# Metrics\n",
    "from keras.metrics import BinaryAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natha\\Documents\\GitHub\\4th-year-Polytech\\Deep Learning\\env_deep_learning\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natha\\Documents\\GitHub\\4th-year-Polytech\\Deep Learning\\env_deep_learning\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 77ms/step - binary_accuracy: 0.6360 - loss: 0.6727 - val_binary_accuracy: 0.8561 - val_loss: 0.3320\n",
      "Epoch 2/3\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 86ms/step - binary_accuracy: 0.9211 - loss: 0.2006 - val_binary_accuracy: 0.8301 - val_loss: 0.4144\n",
      "Epoch 3/3\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 87ms/step - binary_accuracy: 0.9795 - loss: 0.0705 - val_binary_accuracy: 0.8435 - val_loss: 0.4600\n"
     ]
    }
   ],
   "source": [
    "# Dimension de l'espace ou seront projetés les tokens par l'embedding_layer\n",
    "token_projection_dim = 32\n",
    "\n",
    "\n",
    "input_layer = Input(shape = [200,], dtype = 'int64')\n",
    "\n",
    "embedding_layer = Embedding(input_dim = 4000, input_length = 200, output_dim = token_projection_dim)(input_layer)\n",
    "\n",
    "\n",
    "# Transformers\n",
    "\n",
    "# Couche d'attention\n",
    "MHA_layer_T = MultiHeadAttention(num_heads = 2, dropout = 0.1, key_dim = token_projection_dim)(embedding_layer, embedding_layer, embedding_layer)\n",
    "\n",
    "normalization_layer1_T = LayerNormalization(epsilon = 1e-6)(embedding_layer + MHA_layer_T)\n",
    "\n",
    "dense_layer1_T = Dense(token_projection_dim, activation = \"tanh\")(normalization_layer1_T)\n",
    "\n",
    "dense_layer2_T = Dense(token_projection_dim, activation = \"tanh\")(dense_layer1_T)\n",
    "\n",
    "normalization_layer2_T = LayerNormalization(epsilon = 1e-6)(normalization_layer1_T + dense_layer2_T)\n",
    "\n",
    "flatten_layer_T = Flatten()(normalization_layer2_T)\n",
    "\n",
    "\n",
    "# LSTM\n",
    "\n",
    "LSTM_layer_LSTM = LSTM(5)(embedding_layer)\n",
    "\n",
    "dropout_layer_LSTM = Dropout(0.5)(LSTM_layer_LSTM)\n",
    "\n",
    "\n",
    "# CNN\n",
    "\n",
    "conv_layer_CNN = Conv1D(16, 3, input_shape = (200, token_projection_dim))(embedding_layer)\n",
    "\n",
    "dropout_layer_CNN = Dropout(0.5)(conv_layer_CNN)\n",
    "\n",
    "pooling_layer_CNN = MaxPooling1D(2)(dropout_layer_CNN)\n",
    "\n",
    "flatten_layer_CNN = Flatten()(pooling_layer_CNN)\n",
    "\n",
    "\n",
    "# Concatenate\n",
    "\n",
    "concatenate_layer = Concatenate()([flatten_layer_T, dropout_layer_LSTM, flatten_layer_CNN])\n",
    "\n",
    "\n",
    "# Output\n",
    "\n",
    "output_layer = Dense(1, activation = \"sigmoid\")(concatenate_layer)\n",
    "\n",
    "model = Model(inputs = input_layer, outputs = output_layer)\n",
    "\n",
    "model.compile(optimizer = Adam(learning_rate = 0.001), loss = BinaryCrossentropy(), metrics = [BinaryAccuracy()])\n",
    "\n",
    "# Model fitting\n",
    "history = model.fit(x_train_padded, y_train, batch_size = 32, epochs = 3, validation_data = (x_data_val, y_data_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque qu'au bout d'une epoch, l'accuracy pour les données de validations était déja de 85%, et elle n'a pas augmenté au long de l'apprentissage. En revanche, l'accuracy pour les données d'entrainement augmente logiquement tout au long de l'apprentissage.\n",
    "De plus, on remarque que la loss pour les données de validation augmente au long de l'apprentissage : on fait face à du sur-apprentissage, il est donc nécessaire de ne pas augmenté le nombre d'epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je n'ai pas tracé de graphique pour afficher les courbes d'apprentissages en raison du faible nombre d'epoch, ce n'est pas très cohérent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file = \"model.png\", show_shapes = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - binary_accuracy: 0.8512 - loss: 0.4346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.43928125500679016, 0.849839985370636]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_data_test, y_data_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
